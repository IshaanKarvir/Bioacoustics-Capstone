{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d8e2e0",
   "metadata": {},
   "source": [
    "This notebook is used to convert the a given set of WAV files to spectrograms. These spectrograms will be stored in the \"data\" directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c9eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: - \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pluggy==0.13.1=py37h89c1867_4\n",
      "  - conda-forge/linux-64::scikit-image==0.18.1=py37hdc94413_0\n",
      "  - conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0\n",
      "  - conda-forge/linux-64::keyring==22.0.1=py37h89c1867_0\n",
      "  - defaults/linux-64::spyder==3.3.6=py37_0\n",
      "  - conda-forge/noarch::pytest-astropy-header==0.1.2=py_0\n",
      "  - conda-forge/noarch::pytest-doctestplus==0.9.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytest-arraydiff==0.3=py_0\n",
      "  - conda-forge/noarch::typing-extensions==3.7.4.3=0\n",
      "  - conda-forge/noarch::path.py==12.5.0=0\n",
      "  - conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py37h89c1867_4\n",
      "  - conda-forge/noarch::pooch==1.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.7.2=py_0\n",
      "  - conda-forge/noarch::pytest-cov==2.11.1=pyh44b312d_0\n",
      "  - conda-forge/noarch::pytest-mock==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.3.4=py37h0c9df89_0\n",
      "  - conda-forge/noarch::black==20.8b1=py_1\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.1=py37h89c1867_4\n",
      "  - conda-forge/noarch::pytest-astropy==0.8.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytest-filter-subpackage==0.1.1=py_0\n",
      "  - conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py37h89c1867_6\n",
      "  - conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::yarl==1.6.3=py37h5e8e339_1\n",
      "  - conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::pytest-openfiles==0.5.0=py_0\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/linux-64::path==15.1.2=py37h89c1867_0\n",
      "  - conda-forge/noarch::pytest-remotedata==0.3.2=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/noarch::jsonschema==3.2.0=py_2\n",
      "  - conda-forge/linux-64::distributed==2021.2.0=py37h89c1867_0\n",
      "  - conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1\n",
      "  - conda-forge/linux-64::nbconvert==6.0.7=py37h89c1867_3\n",
      "  - conda-forge/linux-64::pytest==6.2.2=py37h89c1867_0\n",
      "  - conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.3.4=py37h89c1867_0\n",
      "  - conda-forge/linux-64::importlib-metadata==3.7.0=py37h89c1867_0\n",
      "  - conda-forge/noarch::requests==2.25.1=pyhd3deb0d_0\n",
      "  - conda-forge/noarch::hypothesis==6.3.2=pyhd8ed1ab_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistenc/ \n",
      "\n",
      "  - conda-forge/noarch::nbclient==0.5.2=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pluggy==0.13.1=py37h89c1867_4\n",
      "  - conda-forge/linux-64::scikit-image==0.18.1=py37hdc94413_0\n",
      "  - conda-forge/noarch::ipywidgets==7.6.3=pyhd3deb0d_0\n",
      "  - conda-forge/linux-64::keyring==22.0.1=py37h89c1867_0\n",
      "  - defaults/linux-64::spyder==3.3.6=py37_0\n",
      "  - conda-forge/noarch::pytest-astropy-header==0.1.2=py_0\n",
      "  - conda-forge/noarch::pytest-doctestplus==0.9.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytest-arraydiff==0.3=py_0\n",
      "  - conda-forge/noarch::typing-extensions==3.7.4.3=0\n",
      "  - conda-forge/noarch::path.py==12.5.0=0\n",
      "  - conda-forge/noarch::dask==2021.2.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::nbformat==5.1.2=pyhd8ed1ab_1\n",
      "  - conda-forge/linux-64::nb_conda==2.2.1=py37h89c1867_4\n",
      "  - conda-forge/noarch::pooch==1.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::anaconda-client==1.7.2=py_0\n",
      "  - conda-forge/noarch::pytest-cov==2.11.1=pyh44b312d_0\n",
      "  - conda-forge/noarch::pytest-mock==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::aioitertools==0.7.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib-base==3.3.4=py37h0c9df89_0\n",
      "  - conda-forge/noarch::black==20.8b1=py_1\n",
      "  - conda-forge/linux-64::widgetsnbextension==3.5.1=py37h89c1867_4\n",
      "  - conda-forge/noarch::pytest-astropy==0.8.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pytest-filter-subpackage==0.1.1=py_0\n",
      "  - conda-forge/noarch::anaconda-project==0.9.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::importlib_metadata==3.7.0=hd8ed1ab_0\n",
      "  - conda-forge/linux-64::jupyter==1.0.0=py37h89c1867_6\n",
      "  - conda-forge/noarch::jupyterlab_server==2.3.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::yarl==1.6.3=py37h5e8e339_1\n",
      "  - conda-forge/noarch::seaborn-base==0.11.1=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::pytest-openfiles==0.5.0=py_0\n",
      "  - conda-forge/noarch::imageio==2.9.0=py_0\n",
      "  - conda-forge/linux-64::path==15.1.2=py37h89c1867_0\n",
      "  - conda-forge/noarch::pytest-remotedata==0.3.2=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::numpydoc==1.1.0=py_1\n",
      "  - conda-forge/noarch::jsonschema==3.2.0=py_2\n",
      "  - conda-forge/linux-64::distributed==2021.2.0=py37h89c1867_0\n",
      "  - conda-forge/noarch::flask==1.1.2=pyh9f0ad1d_0\n",
      "  - conda-forge/noarch::seaborn==0.11.1=hd8ed1ab_1\n",
      "  - conda-forge/linux-64::nbconvert==6.0.7=py37h89c1867_3\n",
      "  - conda-forge/linux-64::pytest==6.2.2=py37h89c1867_0\n",
      "  - conda-forge/noarch::nbclassic==0.2.6=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::sphinx==3.5.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::matplotlib==3.3.4=py37h89c1867_0\n",
      "  - conda-forge/linux-64::importlib-metadata==3.7.0=py37h89c1867_0\n",
      "  - conda-forge/noarch::requests==2.25.1=pyhd3deb0d_0\n",
      "  - conda-forge/noarch::hypothesis==6.3.2=pyhd8ed1ab_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/mxnet_latest_p37\n",
      "\n",
      "  added / updated specs:\n",
      "    - librosa\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    anyio-3.6.1                |   py37h89c1867_0         153 KB  conda-forge\n",
      "    astroid-2.7.3              |   py37h89c1867_0         329 KB  conda-forge\n",
      "    audioread-2.1.9            |   py37h89c1867_3          33 KB  conda-forge\n",
      "    babel-2.10.1               |     pyhd8ed1ab_0         6.7 MB  conda-forge\n",
      "    bokeh-2.3.3                |   py37h89c1867_0         8.2 MB  conda-forge\n",
      "    ca-certificates-2022.5.18.1|       ha878542_0         144 KB  conda-forge\n",
      "    certifi-2022.5.18.1        |   py37h89c1867_0         150 KB  conda-forge\n",
      "    colorama-0.4.4             |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    coverage-5.5               |   py37h5e8e339_0         265 KB  conda-forge\n",
      "    docutils-0.16              |   py37h89c1867_3         739 KB  conda-forge\n",
      "    ffmpeg-4.3.2               |       hca11adc_0        92.0 MB  conda-forge\n",
      "    fsspec-2022.5.0            |     pyhd8ed1ab_0          96 KB  conda-forge\n",
      "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
      "    jupyter_server-1.15.6      |     pyhd8ed1ab_0         233 KB  conda-forge\n",
      "    lame-3.100                 |    h7f98852_1001         496 KB  conda-forge\n",
      "    libflac-1.3.3              |       h9c3ff4c_1         486 KB  conda-forge\n",
      "    librosa-0.9.1              |     pyhd8ed1ab_0         154 KB  conda-forge\n",
      "    libsndfile-1.0.31          |       h9c3ff4c_1         602 KB  conda-forge\n",
      "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
      "    notebook-6.4.12            |     pyha770c72_0         6.3 MB  conda-forge\n",
      "    openh264-2.1.1             |       h780b84a_0         1.5 MB  conda-forge\n",
      "    pillow-8.2.0               |   py37h4600e1f_1         684 KB  conda-forge\n",
      "    pip-22.1.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n",
      "    platformdirs-2.5.1         |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    pylint-2.10.2              |     pyhd8ed1ab_0         255 KB  conda-forge\n",
      "    pysoundfile-0.10.3.post1   |     pyhd3deb0d_0          23 KB  conda-forge\n",
      "    resampy-0.2.2              |             py_0         332 KB  conda-forge\n",
      "    typing_extensions-3.7.4.3  |             py_0          25 KB  conda-forge\n",
      "    websocket-client-1.3.2     |     pyhd8ed1ab_0          41 KB  conda-forge\n",
      "    werkzeug-2.1.2             |     pyhd8ed1ab_1         237 KB  conda-forge\n",
      "    x264-1!161.3030            |       h7f98852_1         2.5 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       132.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  anyio              conda-forge/linux-64::anyio-3.6.1-py37h89c1867_0\n",
      "  astroid            conda-forge/linux-64::astroid-2.7.3-py37h89c1867_0\n",
      "  audioread          conda-forge/linux-64::audioread-2.1.9-py37h89c1867_3\n",
      "  babel              conda-forge/noarch::babel-2.10.1-pyhd8ed1ab_0\n",
      "  bleach             conda-forge/noarch::bleach-5.0.0-pyhd8ed1ab_0\n",
      "  bokeh              conda-forge/linux-64::bokeh-2.3.3-py37h89c1867_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  coverage           conda-forge/linux-64::coverage-5.5-py37h5e8e339_0\n",
      "  dask-core          conda-forge/noarch::dask-core-2021.2.0-pyhd8ed1ab_0\n",
      "  docutils           conda-forge/linux-64::docutils-0.16-py37h89c1867_3\n",
      "  ffmpeg             conda-forge/linux-64::ffmpeg-4.3.2-hca11adc_0\n",
      "  fsspec             conda-forge/noarch::fsspec-2022.5.0-pyhd8ed1ab_0\n",
      "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1\n",
      "  jupyter_server     conda-forge/noarch::jupyter_server-1.15.6-pyhd8ed1ab_0\n",
      "  lame               conda-forge/linux-64::lame-3.100-h7f98852_1001\n",
      "  libflac            conda-forge/linux-64::libflac-1.3.3-h9c3ff4c_1\n",
      "  libogg             conda-forge/linux-64::libogg-1.3.4-h7f98852_1\n",
      "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1\n",
      "  librosa            conda-forge/noarch::librosa-0.9.1-pyhd8ed1ab_0\n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.0.31-h9c3ff4c_1\n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h9c3ff4c_0\n",
      "  msgpack-python     conda-forge/linux-64::msgpack-python-1.0.2-py37h2527ec5_1\n",
      "  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.5-pyhd8ed1ab_0\n",
      "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0\n",
      "  notebook           conda-forge/noarch::notebook-6.4.12-pyha770c72_0\n",
      "  openh264           conda-forge/linux-64::openh264-2.1.1-h780b84a_0\n",
      "  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0\n",
      "  pillow             conda-forge/linux-64::pillow-8.2.0-py37h4600e1f_1\n",
      "  pip                conda-forge/noarch::pip-22.1.2-pyhd8ed1ab_0\n",
      "  platformdirs       conda-forge/noarch::platformdirs-2.5.1-pyhd8ed1ab_0\n",
      "  pylint             conda-forge/noarch::pylint-2.10.2-pyhd8ed1ab_0\n",
      "  pysoundfile        conda-forge/noarch::pysoundfile-0.10.3.post1-pyhd3deb0d_0\n",
      "  resampy            conda-forge/noarch::resampy-0.2.2-py_0\n",
      "  send2trash         conda-forge/noarch::send2trash-1.8.0-pyhd8ed1ab_0\n",
      "  typing_extensions  conda-forge/noarch::typing_extensions-3.7.4.3-py_0\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.9-pyhd8ed1ab_0\n",
      "  websocket-client   conda-forge/noarch::websocket-client-1.3.2-pyhd8ed1ab_0\n",
      "  werkzeug           conda-forge/noarch::werkzeug-2.1.2-pyhd8ed1ab_1\n",
      "  x264               conda-forge/linux-64::x264-1!161.3030-h7f98852_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2021.10.8-ha878542_0 --> 2022.5.18.1-ha878542_0\n",
      "  certifi                          2021.10.8-py37h89c1867_1 --> 2022.5.18.1-py37h89c1867_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "werkzeug-2.1.2       | 237 KB    | ##################################### | 100% \n",
      "ca-certificates-2022 | 144 KB    | ##################################### | 100% \n",
      "audioread-2.1.9      | 33 KB     | ##################################### | 100% \n",
      "fsspec-2022.5.0      | 96 KB     | ##################################### | 100% \n",
      "typing_extensions-3. | 25 KB     | ##################################### | 100% \n",
      "pip-22.1.2           | 1.5 MB    | ##################################### | 100% \n",
      "pylint-2.10.2        | 255 KB    | ##################################### | 100% \n",
      "x264-1!161.3030      | 2.5 MB    | ##################################### | 100% \n",
      "jupyter_server-1.15. | 233 KB    | ##################################### | 100% \n",
      "resampy-0.2.2        | 332 KB    | ##################################### | 100% \n",
      "certifi-2022.5.18.1  | 150 KB    | ##################################### | 100% \n",
      "websocket-client-1.3 | 41 KB     | ##################################### | 100% \n",
      "librosa-0.9.1        | 154 KB    | ##################################### | 100% \n",
      "libflac-1.3.3        | 486 KB    | ##################################### | 100% \n",
      "coverage-5.5         | 265 KB    | ##################################### | 100% \n",
      "anyio-3.6.1          | 153 KB    | ##################################### | 100% \n",
      "notebook-6.4.12      | 6.3 MB    | ##################################### | 100% \n",
      "docutils-0.16        | 739 KB    | ##################################### | 100% \n",
      "nettle-3.6           | 6.5 MB    | ##################################### | 100% \n",
      "bokeh-2.3.3          | 8.2 MB    | ##################################### | 100% \n",
      "openh264-2.1.1       | 1.5 MB    | ##################################### | 100% \n",
      "platformdirs-2.5.1   | 15 KB     | ##################################### | 100% \n",
      "lame-3.100           | 496 KB    | ##################################### | 100% \n",
      "pillow-8.2.0         | 684 KB    | ##################################### | 100% \n",
      "ffmpeg-4.3.2         | 92.0 MB   | ##################################### | 100% \n",
      "gnutls-3.6.13        | 2.0 MB    | ##################################### | 100% \n",
      "libsndfile-1.0.31    | 602 KB    | ##################################### | 100% \n",
      "babel-2.10.1         | 6.7 MB    | ##################################### | 100% \n",
      "colorama-0.4.4       | 18 KB     | ##################################### | 100% \n",
      "astroid-2.7.3        | 329 KB    | ##################################### | 100% \n",
      "pysoundfile-0.10.3.p | 23 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fddfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from mxnet) (2.25.1)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/gpu_cuda10.1/lib/python3.7/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from mxnet) (1.20.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2022.5.18.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.26.9)\n",
      "Installing collected packages: mxnet\n",
      "Successfully installed mxnet-1.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63c02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.io import wavfile\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import json\n",
    "from os.path import exists\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_col, right_col = \"Begin Time (s)\", \"End Time (s)\"\n",
    "top_col, bot_col = \"High Freq (Hz)\", \"Low Freq (Hz)\"\n",
    "class_col, class_conf_col = \"Species\", \"Species Confidence\"\n",
    "\n",
    "recording_dir = \"./\"\n",
    "annotation_dir = \"./\"\n",
    "output_dir = \"./data\"\n",
    "label_map_name = \"label_map.pbtxt\"\n",
    "metadata_name = \"dataset_metadata.txt\"\n",
    "JSON_dir = \"./json\"\n",
    "manifest_file_name = \"manifest_file.jsonl\"\n",
    "lst_file_name = 'lst_file.lst'\n",
    "\n",
    "# SPECTROGRAM CONSTANTS\n",
    "# Window size (n_fft) in seconds\n",
    "WINDOW_SIZE_SEC = 3/20\n",
    "# Hop Length in seconds\n",
    "HOP_LEN_SEC = 15/300\n",
    "# Number of frequency bands (y dimension of spectrogram)\n",
    "N_MELS = 300\n",
    "# Maximum frequency considered (highest value in y dimension)\n",
    "FREQUENCY_MAX = 1600\n",
    "\n",
    "# CHUNK CONSTANTS\n",
    "# Length of one chunk in seconds\n",
    "TRAIN_CHUNK_SIZE_SEC = 45\n",
    "EVAL_CHUNK_SIZE_SEC = 15\n",
    "# Minimum % visibility of a call to keep annotation\n",
    "MIN_BOX_PERCENT = 0.3\n",
    "\n",
    "# DATASET SETTINGS\n",
    "dataset_name = \"dataset.record\"\n",
    "NUM_TRAIN_SHARDS = 3\n",
    "NUM_EVAL_SHARDS = 5\n",
    "NUM_EVAL_FILES = 2\n",
    "\n",
    "# Constructs the dataset without certain classes\n",
    "DISALLOWED_CLASSES = [\"?\", \"rf\", \"sl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if needed\n",
    "with open(path.join(output_dir, metadata_name), 'w') as metafile:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"WINDOW_SIZE_SEC\": WINDOW_SIZE_SEC,\n",
    "            \"HOP_LEN_SEC\": HOP_LEN_SEC,\n",
    "            \"N_MELS\": N_MELS,\n",
    "            \"FREQUENCY_MAX\": FREQUENCY_MAX,\n",
    "            \"TRAIN_CHUNK_SIZE_SEC\": TRAIN_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_SIZE_SEC\": EVAL_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_STEP_SEC\": EVAL_CHUNK_SIZE_SEC / 2.0\n",
    "        },\n",
    "        metafile\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a connection to bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('monitoring-whale-recordings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7912319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in a wav file\n",
    "def read_wavfile(wav_name, normalize=True, verbose=False):\n",
    "    file_name = f\"{wav_name}_processed.wav\"\n",
    "    bucket_path = f\"wav-files/decimated_files/{file_name}\"\n",
    "    bucket.download_file(bucket_path, file_name)\n",
    "    if verbose:\n",
    "        print(\"Reading {}\".format(file_name))\n",
    "    sr, data = wavfile.read(file_name)\n",
    "    os.remove(file_name)\n",
    "    if verbose:\n",
    "        print(\"{} samples at {} samples/sec --> {} seconds\".format(data.shape[0], sr, data.shape[0]/sr))\n",
    "\n",
    "    if normalize:\n",
    "        data = data.astype(float)\n",
    "        data = data - data.min()\n",
    "        data = data / data.max()\n",
    "        data = data - 0.5\n",
    "    return sr, data\n",
    "\n",
    "#Tries to find corresponding annotation file for all of the annotators\n",
    "def read_annotations(fname, verbose=False):\n",
    "    annotators = ['AS.txt', 'AW.txt', 'JW.txt', 'MS.txt', 'SS.txt']\n",
    "    \n",
    "    \n",
    "    for annotator in annotators:\n",
    "        file_name = f\"{fname}-{annotator}\"\n",
    "#         file_name = f\"{fname}-AW.txt\"\n",
    "        bucket_path = f\"selection-tables/{file_name}\"\n",
    "        try:\n",
    "            bucket.download_file(bucket_path, file_name)\n",
    "            break\n",
    "        except Exception:\n",
    "                continue\n",
    "    \n",
    "    annotations = pd.read_csv(file_name, sep=\"\\t\")\n",
    "    try:\n",
    "        annotations = annotations.loc[annotations[\"Species\"] == \"hb\"]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        annotations = annotations.loc[annotations[\"Spcies\"] == \"hb\"]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Read {} annotations from {}\".format(len(annotations), fname))\n",
    "        print(\"Columns:\", \",\".join([\" {} ({})\".format(c, type(c)) for c in annotations.columns]))\n",
    "    os.remove(file_name)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f51613",
   "metadata": {},
   "source": [
    "This section includes a function that creates the training, testing, and validation set we used for our training. It also includes an \"incorrect_dataset\" which contains the names of wav files that had something wrong with them that was causing problems. The function below pulls all the wav files from the \"monitoring-whale-recordings\". It then removes the hardcoded wav files reserved for testing and validation purposes. The remaining files become the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b576c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the data sets. The train and validation sets have ben hardcoded, feel free to modify.\n",
    "def get_data_sets():\n",
    "    testing_set = ['671658014.181008003414']\n",
    "\n",
    "    #dataset with misspelled columns\n",
    "    incorrect_dataset = ['671658014.181003123500']\n",
    "\n",
    "    validation_set = ['671658014.181008033412']\n",
    "\n",
    "    a = s3.Bucket('monitoring-whale-recordings')\n",
    "    annotatedFiles = [file.key.split(\"/\")[1] for file in a.objects.all() if (file.key[-1] != '/' and file.key.split(\"/\")[0] == \"selection-tables\")]\n",
    "    dataset = [file.split(\"-\")[0] for file in annotatedFiles]\n",
    "#     train_dataset = [el for el in dataset if not el in validation_set and not el in testing_set]\n",
    "    notAllowedSet = testing_set + incorrect_dataset + validation_set\n",
    "    train_set = [file for file in dataset if all(file not in notAllowed for notAllowed in notAllowedSet)]\n",
    "    \n",
    "    return train_set, incorrect_dataset, validation_set, testing_set\n",
    "\n",
    "train_set, incorrect_dataset, validation_set, testing_set = get_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a41596",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783dbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classes(annotation_fnames, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all classes seen in the annotation files sorted\n",
    "    alphabetically.\n",
    "    \"\"\"\n",
    "    classes = set()\n",
    "    for annot_fname in annotation_fnames:\n",
    "        try:\n",
    "            classes.update(list(read_annotations(annot_fname)[class_col].unique()))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(classes)\n",
    "    classes = sorted([s for s in list(classes)])\n",
    "    if verbose:\n",
    "        print(\"Classes: \", classes)\n",
    "    return classes\n",
    "\n",
    "\n",
    "# Generates the necessary prototext file for the class mapping.\n",
    "# Classes are assigned to the integer 1 greater than their index.\n",
    "# The resulting file is saved to output_path.\n",
    "def create_label_map(classes, output_path):\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    for i, cls in enumerate(classes):\n",
    "        new_item = label_map.item.add() # StringIntLabelMapItem\n",
    "        new_item.name = cls          # String name. The most common practice is to set this to a MID or synsets id.\n",
    "        new_item.id = 1+i            # Integer id starting from 1\n",
    "        new_item.display_name = cls  # Human readable text label\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(text_format.MessageToString(label_map))\n",
    "        \n",
    "\n",
    "classes = get_all_classes(train_set, verbose=True)\n",
    "classes = [c for c in classes if c not in DISALLOWED_CLASSES]\n",
    "    \n",
    "\n",
    "class_map = {}\n",
    "rev_class_map = {}\n",
    "for i in range(len(classes)):\n",
    "    class_map[i+1] = classes[i]\n",
    "    rev_class_map[classes[i]] = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(annotation):\n",
    "    return ((annotation[right_col] - annotation[left_col])\n",
    "            * (annotation[top_col] - annotation[bot_col]))\n",
    "\n",
    "\n",
    "# Per-channel energy normalization\n",
    "def PCEN(spec, M_return_timestep, init_val=None, epsilon=1e-6, s=0.001, alpha=0.80, delta=2.0, r=0.5):\n",
    "    output = np.zeros_like(spec)\n",
    "    if M_return_timestep < 0 or M_return_timestep > spec.shape[1]-1:\n",
    "        print(\"Warning! M return timestep is outside bounds. Not returning any M.\")\n",
    "    if init_val is None:\n",
    "        M = np.zeros(shape=(output.shape[0]))\n",
    "    else:\n",
    "        M = np.array(init_val)\n",
    "    assert M.shape[0] == output.shape[0]\n",
    "    out_M = None\n",
    "    for t in range(output.shape[1]):\n",
    "        M = (1 - s) * M + s * spec[:,t]\n",
    "        output[:,t] = ((spec[:,t] / ((M + epsilon) ** alpha)) ** r) - (delta ** r)\n",
    "        if t == M_return_timestep:\n",
    "            out_M = M\n",
    "    return output, out_M\n",
    "\n",
    "\n",
    "# Returns the min and max db observed in all wav files\n",
    "def get_minmax_bounds(wav_filenames, chunk_size=TRAIN_CHUNK_SIZE_SEC):\n",
    "    min_val, max_val = None, None\n",
    "    for wfname in wav_filenames:\n",
    "        sr, data = read_wavfile(wfname, normalize=True)\n",
    "        n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "        hop_len = int(HOP_LEN_SEC * sr)\n",
    "        chunk_size = int(chunk_size * sr)\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "        M_init = None\n",
    "        for start_i in range(0, len(data), step):\n",
    "            mel_spec = librosa.feature.melspectrogram(y=data[start_i:min(len(data),start_i+chunk_size)],\n",
    "                                                      sr=sr,\n",
    "                                                      n_fft=n_fft,\n",
    "                                                      hop_length=hop_len,\n",
    "                                                      n_mels=N_MELS,\n",
    "                                                      fmax=FREQUENCY_MAX,\n",
    "                                                      center=False)\n",
    "            #mel_spec, M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            temp_min = mel_spec.min()\n",
    "            temp_max = mel_spec.max()\n",
    "            if min_val is None or temp_min < min_val:\n",
    "                min_val = temp_min\n",
    "            if max_val is None or temp_max > max_val:\n",
    "                max_val = temp_max\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "#this is the function that creates the spectrograms and the lst file\n",
    "def process_file(wav_filename, annot_filename, min_bound, max_bound, chunk_size, lst_file_name, chunk_layout=\"dense\",\n",
    "                 drop_last_chunk=False, verbose=False):\n",
    "    sr, data = read_wavfile(wav_filename, normalize=True, verbose=verbose)\n",
    "    annotations = read_annotations(annot_filename, verbose=verbose)\n",
    "    \n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_len = int(HOP_LEN_SEC * sr)\n",
    "    chunk_size = int(chunk_size * sr)\n",
    "    \n",
    "    if chunk_layout == \"dense\":\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "    elif chunk_layout == \"sparse\":\n",
    "        step = chunk_size // 2\n",
    "    \n",
    "    # Start Indices of each chunk\n",
    "    start_vals = [s for s in range(0, len(data), step)]\n",
    "    \n",
    "    # If last cut point creates a tiny chunk, remove it\n",
    "    if len(data) - start_vals[-1] < int(chunk_size / 2):\n",
    "        start_vals = start_vals[:-1]\n",
    "        \n",
    "\n",
    "    def extract_chunk(start_i, end_i, spec_name, annot_name, json_name, index, use_pcen=True, M_init=None):\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data[start_i:end_i],\n",
    "                                                  sr=sr,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  hop_length=hop_len,\n",
    "                                                  n_mels=N_MELS,\n",
    "                                                  fmax=FREQUENCY_MAX,\n",
    "                                                  center=False)\n",
    "        #mel_spec, next_M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "        next_M_init = None\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec = np.clip((mel_spec - min_bound) / (max_bound - min_bound) * 255, a_min=0, a_max=255)\n",
    "        mel_spec = mel_spec.astype(np.uint8)\n",
    "        spec_height, spec_width = mel_spec.shape\n",
    "\n",
    "\n",
    "        # Get annotations to those inside chunk\n",
    "        start_s, end_s = start_i/sr, end_i/sr\n",
    "        freq_axis_low, freq_axis_high = librosa.hz_to_mel(0.0), librosa.hz_to_mel(FREQUENCY_MAX)\n",
    "        chunk_annotations = annotations.loc[~((annotations[left_col] > end_s)\n",
    "                                              | (annotations[right_col] < start_s))].copy()\n",
    "        print(start_s, end_s)\n",
    "\n",
    "    #         createJSON(chunk_annotations)\n",
    "\n",
    "        # Rescale axes to 0.0-1.0 based on location inside chunk\n",
    "        chunk_annotations.loc[:,[left_col,right_col]] = ((chunk_annotations[[left_col,right_col]]\n",
    "                                                         - start_s) / (end_s - start_s))\n",
    "\n",
    "        chunk_annotations.loc[:,[bot_col,top_col]] = (1.0 - ((librosa.hz_to_mel(chunk_annotations[[bot_col,top_col]])\n",
    "                                                      - freq_axis_low) / (freq_axis_high - freq_axis_low)))\n",
    "        chunk_annotations = chunk_annotations.loc[chunk_annotations[class_col].isin(classes)]\n",
    "        trimmed_annots = chunk_annotations.copy()\n",
    "        trimmed_annots[left_col] = trimmed_annots[left_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[right_col] = trimmed_annots[right_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[bot_col] = trimmed_annots[bot_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[top_col] = trimmed_annots[top_col].clip(lower=0, upper=1.0)\n",
    "\n",
    "\n",
    "\n",
    "        overlaps = []\n",
    "        for i in trimmed_annots.index:\n",
    "            intersection = trimmed_annots.loc[i]\n",
    "            original = chunk_annotations.loc[i]\n",
    "            original_area = get_area(original)\n",
    "            overlaps.append((get_area(intersection)*spec_height*spec_width) / original_area)\n",
    "        chunk_annotations = trimmed_annots.loc[np.array(overlaps) > MIN_BOX_PERCENT]\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Found {} annotations in chunk\".format(len(chunk_annotations)))\n",
    "\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Saved spectrogram to '{}'\".format(spec_name))\n",
    "\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        example_dict = {\n",
    "            \"filepath\": spec_name,\n",
    "            \"height\": spec_height,\n",
    "            \"width\": spec_width,\n",
    "            \"xmins\": trimmed_annots[left_col].tolist(),\n",
    "            \"xmaxs\": trimmed_annots[right_col].tolist(),\n",
    "            \"ymins\": trimmed_annots[top_col].tolist(),\n",
    "            \"ymaxs\": trimmed_annots[bot_col].tolist(),\n",
    "            \"classes_text\": trimmed_annots[class_col].tolist(),\n",
    "            \"classes\": trimmed_annots[class_col].map(rev_class_map).tolist()\n",
    "        }\n",
    "    #     annots = createJSON(example_dict)\n",
    "\n",
    "#             Save Chunk as PNG image (lossless compression)\n",
    "        im = Image.fromarray(mel_spec[::-1, :])\n",
    "        im = im.convert(\"L\")\n",
    "\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        im.save(image_filepath)\n",
    "\n",
    "        if(len(example_dict[\"xmins\"]) == 0):\n",
    "            return example_dict, next_M_init\n",
    "        \n",
    "        if(len(example_dict[\"xmins\"]) == 0):\n",
    "            print(index)\n",
    "        res = [index, 2, 5]\n",
    "        for i in range(len(example_dict[\"xmins\"])):\n",
    "            temp = [0, example_dict[\"xmins\"][i], example_dict[\"ymins\"][i], example_dict[\"xmaxs\"][i], example_dict[\"ymaxs\"][i]]\n",
    "            res.extend(temp)\n",
    "        \n",
    "        res.append(image_filepath) \n",
    "\n",
    "        text = \"\\t\".join([str(el) for el in res])\n",
    "        with open(lst_file_name, \"a\") as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "        return example_dict, next_M_init\n",
    "    \n",
    "    \n",
    "    # Actually iterate through the file and extract chunks\n",
    "    examples = []\n",
    "    M_init = None\n",
    "    for ind, start_i in enumerate(start_vals[:-1]):\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, ind)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, ind)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, M_init = extract_chunk(start_i, start_i+chunk_size, spec_name, annot_name, json_name, ind, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    if not drop_last_chunk:\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, len(start_vals)-1)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, len(start_vals)-1)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, _ = extract_chunk(start_vals[-1], len(data), spec_name, annot_name, json_name, len(start_vals)-1, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d241b56",
   "metadata": {},
   "source": [
    "This function removes all the spectrograms from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    !rm data/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83483764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lst_file(dataset, lst_file_name):\n",
    "    index = 0\n",
    "    for file in dataset:\n",
    "    #     if index > 0:\n",
    "    #         break\n",
    "        print(f\"{index + 1}/{len(dataset)} wav files converted\")\n",
    "        index += 1\n",
    "        process_file(file, file, -80.0, 0, TRAIN_CHUNK_SIZE_SEC, lst_file_name,chunk_layout=\"dense\", drop_last_chunk=False, verbose=False)\n",
    "\n",
    "#takes in a data set, and then creates the sepctrograms and the corresponding rec file. \n",
    "#Make sure to call cleanup before every call to this function\n",
    "def create_rec_file(lst_file_name):\n",
    "    RESIZE_SIZE = 256\n",
    "    !python im2rec.py --resize $RESIZE_SIZE --pack-label $lst_file_name .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_Lst_fileIfOpen(file_name):\n",
    "    if exists(file_name):\n",
    "        print(f\"{file_name} exists, removing now\")\n",
    "        !rm $file_name\n",
    "\n",
    "    \n",
    "#copies file from local notebook instance to sagemaker bucket\n",
    "def copy_to_bucket(fileSource, fileDestination):\n",
    "    #copies it into our bucket\n",
    "    write_bucket = s3.Bucket('sagemaker-us-west-2-959616474350')\n",
    "    write_bucket.upload_file(fileSource, fileDestination)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61a336",
   "metadata": {},
   "source": [
    "This functions takes in a list of WAV file names, and the name of the rec file the annotations need to be stored in, and then creates the corresponding spectrograms and rec file for the WAV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbd03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_rec_file(dataset, file_prefix):\n",
    "    lst_file_name = f\"{file_prefix}.lst\"\n",
    "    remove_Lst_fileIfOpen(lst_file_name)\n",
    "    create_lst_file(dataset, lst_file_name)\n",
    "    create_rec_file(lst_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c50fcd",
   "metadata": {},
   "source": [
    "Remember to call cleanup before any call to final_rec_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dd913f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all the spectrograms in the data folder\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2edfb8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_full.lst exists, removing now\n",
      "1/26\n",
      "2/26\n",
      "3/26\n",
      "4/26\n",
      "5/26\n",
      "6/26\n",
      "7/26\n",
      "8/26\n",
      "9/26\n",
      "10/26\n",
      "11/26\n",
      "12/26\n",
      "13/26\n",
      "14/26\n",
      "15/26\n",
      "16/26\n",
      "17/26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0f272765b2f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#creates spectrograms and rec file for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_rec_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a4ebfa5d3fb6>\u001b[0m in \u001b[0;36mfinal_rec_file\u001b[0;34m(dataset, file_prefix)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlst_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{file_prefix}.lst\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mremove_Lst_fileIfOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcreate_lst_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcreate_rec_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-afd2d88843da>\u001b[0m in \u001b[0;36mcreate_lst_file\u001b[0;34m(dataset, lst_file_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{index + 1}/{len(dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m80.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_CHUNK_SIZE_SEC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlst_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk_layout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dense\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last_chunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#takes in a data set, and then creates the sepctrograms and the corresponding rec file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-16051da5c17c>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(wav_filename, annot_filename, min_bound, max_bound, chunk_size, lst_file_name, chunk_layout, drop_last_chunk, verbose)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mannot_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}-{}-labels.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mjson_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{wav_filename}.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdrop_last_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-16051da5c17c>\u001b[0m in \u001b[0;36mextract_chunk\u001b[0;34m(start_i, end_i, spec_name, annot_name, json_name, index, use_pcen, M_init)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m#mel_spec, next_M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mnext_M_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mmel_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mmel_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_spec\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_bound\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_bound\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mmel_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mpower_to_db\u001b[0;34m(S, ref, amin, top_db)\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0mref_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0mlog_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m     \u001b[0mlog_spec\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m10.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#creates spectrograms and rec file\n",
    "final_rec_file(train_set, \"train_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copies rec file from here to the bucket\n",
    "copy_to_bucket(\"train_full.rec\", \"train/train.rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ed1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78621667",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rec_file(validation_set, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17c16ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_bucket(\"val.rec\", \"validation/validation.rec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
