{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0027e3e8",
   "metadata": {},
   "source": [
    "This notebook is used to convert the a given set of WAV files to spectrograms. These spectrograms will be stored in the \"data\" directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.io import wavfile\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import json\n",
    "from os.path import exists\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_col, right_col = \"Begin Time (s)\", \"End Time (s)\"\n",
    "top_col, bot_col = \"High Freq (Hz)\", \"Low Freq (Hz)\"\n",
    "class_col, class_conf_col = \"Species\", \"Species Confidence\"\n",
    "\n",
    "recording_dir = \"./\"\n",
    "annotation_dir = \"./\"\n",
    "output_dir = \"./data\"\n",
    "label_map_name = \"label_map.pbtxt\"\n",
    "metadata_name = \"dataset_metadata.txt\"\n",
    "JSON_dir = \"./json\"\n",
    "manifest_file_name = \"manifest_file.jsonl\"\n",
    "lst_file_name = 'lst_file.lst'\n",
    "\n",
    "# SPECTROGRAM CONSTANTS\n",
    "# Window size (n_fft) in seconds\n",
    "WINDOW_SIZE_SEC = 3/20\n",
    "# Hop Length in seconds\n",
    "HOP_LEN_SEC = 15/300\n",
    "# Number of frequency bands (y dimension of spectrogram)\n",
    "N_MELS = 300\n",
    "# Maximum frequency considered (highest value in y dimension)\n",
    "FREQUENCY_MAX = 1600\n",
    "\n",
    "# CHUNK CONSTANTS\n",
    "# Length of one chunk in seconds\n",
    "TRAIN_CHUNK_SIZE_SEC = 45\n",
    "EVAL_CHUNK_SIZE_SEC = 15\n",
    "# Minimum % visibility of a call to keep annotation\n",
    "MIN_BOX_PERCENT = 0.3\n",
    "\n",
    "# DATASET SETTINGS\n",
    "dataset_name = \"dataset.record\"\n",
    "NUM_TRAIN_SHARDS = 3\n",
    "NUM_EVAL_SHARDS = 5\n",
    "NUM_EVAL_FILES = 2\n",
    "\n",
    "# Constructs the dataset without certain classes\n",
    "DISALLOWED_CLASSES = [\"?\", \"rf\", \"sl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da976c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if needed\n",
    "with open(path.join(output_dir, metadata_name), 'w') as metafile:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"WINDOW_SIZE_SEC\": WINDOW_SIZE_SEC,\n",
    "            \"HOP_LEN_SEC\": HOP_LEN_SEC,\n",
    "            \"N_MELS\": N_MELS,\n",
    "            \"FREQUENCY_MAX\": FREQUENCY_MAX,\n",
    "            \"TRAIN_CHUNK_SIZE_SEC\": TRAIN_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_SIZE_SEC\": EVAL_CHUNK_SIZE_SEC,\n",
    "            \"EVAL_CHUNK_STEP_SEC\": EVAL_CHUNK_SIZE_SEC / 2.0\n",
    "        },\n",
    "        metafile\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a connection to bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('monitoring-whale-recordings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in a wav file\n",
    "def read_wavfile(wav_name, normalize=True, verbose=False):\n",
    "    file_name = f\"{wav_name}_processed.wav\"\n",
    "    bucket_path = f\"wav-files/decimated_files/{file_name}\"\n",
    "    bucket.download_file(bucket_path, file_name)\n",
    "    if verbose:\n",
    "        print(\"Reading {}\".format(file_name))\n",
    "    sr, data = wavfile.read(file_name)\n",
    "    os.remove(file_name)\n",
    "    if verbose:\n",
    "        print(\"{} samples at {} samples/sec --> {} seconds\".format(data.shape[0], sr, data.shape[0]/sr))\n",
    "\n",
    "    if normalize:\n",
    "        data = data.astype(float)\n",
    "        data = data - data.min()\n",
    "        data = data / data.max()\n",
    "        data = data - 0.5\n",
    "    return sr, data\n",
    "\n",
    "#Tries to find corresponding annotation file for all of the annotators\n",
    "def read_annotations(fname, verbose=False):\n",
    "    annotators = ['AS.txt', 'AW.txt', 'JW.txt', 'MS.txt', 'SS.txt']\n",
    "    \n",
    "    \n",
    "    for annotator in annotators:\n",
    "        file_name = f\"{fname}-{annotator}\"\n",
    "#         file_name = f\"{fname}-AW.txt\"\n",
    "        bucket_path = f\"selection-tables/{file_name}\"\n",
    "        try:\n",
    "            bucket.download_file(bucket_path, file_name)\n",
    "            break\n",
    "        except Exception:\n",
    "                continue\n",
    "    \n",
    "    annotations = pd.read_csv(file_name, sep=\"\\t\")\n",
    "    try:\n",
    "        annotations = annotations.loc[annotations[\"Species\"] == \"hb\"]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        annotations = annotations.loc[annotations[\"Spcies\"] == \"hb\"]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Read {} annotations from {}\".format(len(annotations), fname))\n",
    "        print(\"Columns:\", \",\".join([\" {} ({})\".format(c, type(c)) for c in annotations.columns]))\n",
    "    os.remove(file_name)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a21e4",
   "metadata": {},
   "source": [
    "This section includes a function that creates the training, testing, and validation set we used for our training. It also includes an \"incorrect_dataset\" which contains the names of wav files that had something wrong with them that was causing problems. The function below pulls all the wav files from the \"monitoring-whale-recordings\". It then removes the hardcoded wav files reserved for testing and validation purposes. The remaining files become the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all of the data sets. The train and validation sets have ben hardcoded, feel free to modify.\n",
    "def get_data_sets():\n",
    "    testing_set = ['671658014.181008003414']\n",
    "\n",
    "    #dataset with misspelled columns\n",
    "    incorrect_dataset = ['671658014.181003123500']\n",
    "\n",
    "    validation_set = ['671658014.181008033412']\n",
    "\n",
    "    a = s3.Bucket('monitoring-whale-recordings')\n",
    "    annotatedFiles = [file.key.split(\"/\")[1] for file in a.objects.all() if (file.key[-1] != '/' and file.key.split(\"/\")[0] == \"selection-tables\")]\n",
    "    dataset = [file.split(\"-\")[0] for file in annotatedFiles]\n",
    "#     train_dataset = [el for el in dataset if not el in validation_set and not el in testing_set]\n",
    "    notAllowedSet = testing_set + incorrect_dataset + validation_set\n",
    "    train_set = [file for file in dataset if all(file not in notAllowed for notAllowed in notAllowedSet)]\n",
    "    \n",
    "    return train_set, incorrect_dataset, validation_set, testing_set\n",
    "\n",
    "train_set, incorrect_dataset, validation_set, testing_set = get_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classes(annotation_fnames, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns a list of all classes seen in the annotation files sorted\n",
    "    alphabetically.\n",
    "    \"\"\"\n",
    "    classes = set()\n",
    "    for annot_fname in annotation_fnames:\n",
    "        try:\n",
    "            classes.update(list(read_annotations(annot_fname)[class_col].unique()))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(classes)\n",
    "    classes = sorted([s for s in list(classes)])\n",
    "    if verbose:\n",
    "        print(\"Classes: \", classes)\n",
    "    return classes\n",
    "\n",
    "\n",
    "# Generates the necessary prototext file for the class mapping.\n",
    "# Classes are assigned to the integer 1 greater than their index.\n",
    "# The resulting file is saved to output_path.\n",
    "def create_label_map(classes, output_path):\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    for i, cls in enumerate(classes):\n",
    "        new_item = label_map.item.add() # StringIntLabelMapItem\n",
    "        new_item.name = cls          # String name. The most common practice is to set this to a MID or synsets id.\n",
    "        new_item.id = 1+i            # Integer id starting from 1\n",
    "        new_item.display_name = cls  # Human readable text label\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(text_format.MessageToString(label_map))\n",
    "        \n",
    "\n",
    "classes = get_all_classes(train_set, verbose=True)\n",
    "classes = [c for c in classes if c not in DISALLOWED_CLASSES]\n",
    "    \n",
    "\n",
    "class_map = {}\n",
    "rev_class_map = {}\n",
    "for i in range(len(classes)):\n",
    "    class_map[i+1] = classes[i]\n",
    "    rev_class_map[classes[i]] = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d84d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(annotation):\n",
    "    return ((annotation[right_col] - annotation[left_col])\n",
    "            * (annotation[top_col] - annotation[bot_col]))\n",
    "\n",
    "\n",
    "# Per-channel energy normalization\n",
    "def PCEN(spec, M_return_timestep, init_val=None, epsilon=1e-6, s=0.001, alpha=0.80, delta=2.0, r=0.5):\n",
    "    output = np.zeros_like(spec)\n",
    "    if M_return_timestep < 0 or M_return_timestep > spec.shape[1]-1:\n",
    "        print(\"Warning! M return timestep is outside bounds. Not returning any M.\")\n",
    "    if init_val is None:\n",
    "        M = np.zeros(shape=(output.shape[0]))\n",
    "    else:\n",
    "        M = np.array(init_val)\n",
    "    assert M.shape[0] == output.shape[0]\n",
    "    out_M = None\n",
    "    for t in range(output.shape[1]):\n",
    "        M = (1 - s) * M + s * spec[:,t]\n",
    "        output[:,t] = ((spec[:,t] / ((M + epsilon) ** alpha)) ** r) - (delta ** r)\n",
    "        if t == M_return_timestep:\n",
    "            out_M = M\n",
    "    return output, out_M\n",
    "\n",
    "\n",
    "# Returns the min and max db observed in all wav files\n",
    "def get_minmax_bounds(wav_filenames, chunk_size=TRAIN_CHUNK_SIZE_SEC):\n",
    "    min_val, max_val = None, None\n",
    "    for wfname in wav_filenames:\n",
    "        sr, data = read_wavfile(wfname, normalize=True)\n",
    "        n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "        hop_len = int(HOP_LEN_SEC * sr)\n",
    "        chunk_size = int(chunk_size * sr)\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "        M_init = None\n",
    "        for start_i in range(0, len(data), step):\n",
    "            mel_spec = librosa.feature.melspectrogram(y=data[start_i:min(len(data),start_i+chunk_size)],\n",
    "                                                      sr=sr,\n",
    "                                                      n_fft=n_fft,\n",
    "                                                      hop_length=hop_len,\n",
    "                                                      n_mels=N_MELS,\n",
    "                                                      fmax=FREQUENCY_MAX,\n",
    "                                                      center=False)\n",
    "            #mel_spec, M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            temp_min = mel_spec.min()\n",
    "            temp_max = mel_spec.max()\n",
    "            if min_val is None or temp_min < min_val:\n",
    "                min_val = temp_min\n",
    "            if max_val is None or temp_max > max_val:\n",
    "                max_val = temp_max\n",
    "    return min_val, max_val\n",
    "\n",
    "\n",
    "#this is the function that creates the spectrograms and the lst file\n",
    "def process_file(wav_filename, annot_filename, min_bound, max_bound, chunk_size, lst_file_name, chunk_layout=\"dense\",\n",
    "                 drop_last_chunk=False, verbose=False):\n",
    "    sr, data = read_wavfile(wav_filename, normalize=True, verbose=verbose)\n",
    "    annotations = read_annotations(annot_filename, verbose=verbose)\n",
    "    \n",
    "    n_fft = int(WINDOW_SIZE_SEC * sr)\n",
    "    hop_len = int(HOP_LEN_SEC * sr)\n",
    "    chunk_size = int(chunk_size * sr)\n",
    "    \n",
    "    if chunk_layout == \"dense\":\n",
    "        step = chunk_size - (hop_len * (N_MELS-2) + n_fft)\n",
    "    elif chunk_layout == \"sparse\":\n",
    "        step = chunk_size // 2\n",
    "    \n",
    "    # Start Indices of each chunk\n",
    "    start_vals = [s for s in range(0, len(data), step)]\n",
    "    \n",
    "    # If last cut point creates a tiny chunk, remove it\n",
    "    if len(data) - start_vals[-1] < int(chunk_size / 2):\n",
    "        start_vals = start_vals[:-1]\n",
    "        \n",
    "\n",
    "    def extract_chunk(start_i, end_i, spec_name, annot_name, json_name, index, use_pcen=True, M_init=None):\n",
    "        mel_spec = librosa.feature.melspectrogram(y=data[start_i:end_i],\n",
    "                                                  sr=sr,\n",
    "                                                  n_fft=n_fft,\n",
    "                                                  hop_length=hop_len,\n",
    "                                                  n_mels=N_MELS,\n",
    "                                                  fmax=FREQUENCY_MAX,\n",
    "                                                  center=False)\n",
    "        #mel_spec, next_M_init = PCEN(mel_spec, step // hop_len, init_val=M_init)\n",
    "        next_M_init = None\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec = np.clip((mel_spec - min_bound) / (max_bound - min_bound) * 255, a_min=0, a_max=255)\n",
    "        mel_spec = mel_spec.astype(np.uint8)\n",
    "        spec_height, spec_width = mel_spec.shape\n",
    "\n",
    "\n",
    "        # Get annotations to those inside chunk\n",
    "        start_s, end_s = start_i/sr, end_i/sr\n",
    "        freq_axis_low, freq_axis_high = librosa.hz_to_mel(0.0), librosa.hz_to_mel(FREQUENCY_MAX)\n",
    "        chunk_annotations = annotations.loc[~((annotations[left_col] > end_s)\n",
    "                                              | (annotations[right_col] < start_s))].copy()\n",
    "        print(start_s, end_s)\n",
    "\n",
    "    #         createJSON(chunk_annotations)\n",
    "\n",
    "        # Rescale axes to 0.0-1.0 based on location inside chunk\n",
    "        chunk_annotations.loc[:,[left_col,right_col]] = ((chunk_annotations[[left_col,right_col]]\n",
    "                                                         - start_s) / (end_s - start_s))\n",
    "\n",
    "        chunk_annotations.loc[:,[bot_col,top_col]] = (1.0 - ((librosa.hz_to_mel(chunk_annotations[[bot_col,top_col]])\n",
    "                                                      - freq_axis_low) / (freq_axis_high - freq_axis_low)))\n",
    "        chunk_annotations = chunk_annotations.loc[chunk_annotations[class_col].isin(classes)]\n",
    "        trimmed_annots = chunk_annotations.copy()\n",
    "        trimmed_annots[left_col] = trimmed_annots[left_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[right_col] = trimmed_annots[right_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[bot_col] = trimmed_annots[bot_col].clip(lower=0, upper=1.0)\n",
    "        trimmed_annots[top_col] = trimmed_annots[top_col].clip(lower=0, upper=1.0)\n",
    "\n",
    "\n",
    "\n",
    "        overlaps = []\n",
    "        for i in trimmed_annots.index:\n",
    "            intersection = trimmed_annots.loc[i]\n",
    "            original = chunk_annotations.loc[i]\n",
    "            original_area = get_area(original)\n",
    "            overlaps.append((get_area(intersection)*spec_height*spec_width) / original_area)\n",
    "        chunk_annotations = trimmed_annots.loc[np.array(overlaps) > MIN_BOX_PERCENT]\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Found {} annotations in chunk\".format(len(chunk_annotations)))\n",
    "\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Saved spectrogram to '{}'\".format(spec_name))\n",
    "\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        example_dict = {\n",
    "            \"filepath\": spec_name,\n",
    "            \"height\": spec_height,\n",
    "            \"width\": spec_width,\n",
    "            \"xmins\": trimmed_annots[left_col].tolist(),\n",
    "            \"xmaxs\": trimmed_annots[right_col].tolist(),\n",
    "            \"ymins\": trimmed_annots[top_col].tolist(),\n",
    "            \"ymaxs\": trimmed_annots[bot_col].tolist(),\n",
    "            \"classes_text\": trimmed_annots[class_col].tolist(),\n",
    "            \"classes\": trimmed_annots[class_col].map(rev_class_map).tolist()\n",
    "        }\n",
    "    #     annots = createJSON(example_dict)\n",
    "\n",
    "#             Save Chunk as PNG image (lossless compression)\n",
    "        im = Image.fromarray(mel_spec[::-1, :])\n",
    "        im = im.convert(\"L\")\n",
    "\n",
    "        image_filepath = path.join(output_dir, spec_name)\n",
    "        im.save(image_filepath)\n",
    "\n",
    "        if(len(example_dict[\"xmins\"]) == 0):\n",
    "            return example_dict, next_M_init\n",
    "        \n",
    "        if(len(example_dict[\"xmins\"]) == 0):\n",
    "            print(index)\n",
    "        res = [index, 2, 5]\n",
    "        for i in range(len(example_dict[\"xmins\"])):\n",
    "            temp = [0, example_dict[\"xmins\"][i], example_dict[\"ymins\"][i], example_dict[\"xmaxs\"][i], example_dict[\"ymaxs\"][i]]\n",
    "            res.extend(temp)\n",
    "        \n",
    "        res.append(image_filepath) \n",
    "\n",
    "        text = \"\\t\".join([str(el) for el in res])\n",
    "        with open(lst_file_name, \"a\") as f:\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n",
    "        return example_dict, next_M_init\n",
    "    \n",
    "    \n",
    "    # Actually iterate through the file and extract chunks\n",
    "    examples = []\n",
    "    M_init = None\n",
    "    for ind, start_i in enumerate(start_vals[:-1]):\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, ind)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, ind)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, M_init = extract_chunk(start_i, start_i+chunk_size, spec_name, annot_name, json_name, ind, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    if not drop_last_chunk:\n",
    "        spec_name = \"{}-{}.png\".format(wav_filename, len(start_vals)-1)\n",
    "        annot_name = \"{}-{}-labels.txt\".format(wav_filename, len(start_vals)-1)\n",
    "        json_name = f\"{wav_filename}.jsonl\"\n",
    "        ex, _ = extract_chunk(start_vals[-1], len(data), spec_name, annot_name, json_name, len(start_vals)-1, M_init=M_init)\n",
    "        examples.append(ex)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86beb025",
   "metadata": {},
   "source": [
    "This function removes all the spectrograms from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    !rm data/*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317aae49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lst_file(dataset, lst_file_name):\n",
    "    index = 0\n",
    "    for file in dataset:\n",
    "    #     if index > 0:\n",
    "    #         break\n",
    "        print(f\"{index + 1}/{len(dataset)} wav files converted\")\n",
    "        index += 1\n",
    "        process_file(file, file, -80.0, 0, TRAIN_CHUNK_SIZE_SEC, lst_file_name,chunk_layout=\"dense\", drop_last_chunk=False, verbose=False)\n",
    "\n",
    "#takes in a data set, and then creates the sepctrograms and the corresponding rec file. \n",
    "#Make sure to call cleanup before every call to this function\n",
    "def create_rec_file(lst_file_name):\n",
    "    RESIZE_SIZE = 256\n",
    "    !python im2rec.py --resize $RESIZE_SIZE --pack-label $lst_file_name .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_Lst_fileIfOpen(file_name):\n",
    "    if exists(file_name):\n",
    "        print(f\"{file_name} exists, removing now\")\n",
    "        !rm $file_name\n",
    "\n",
    "    \n",
    "#copies file from local notebook instance to sagemaker bucket\n",
    "def copy_to_bucket(fileSource, fileDestination):\n",
    "    #copies it into our bucket\n",
    "    write_bucket = s3.Bucket('sagemaker-us-west-2-959616474350')\n",
    "    write_bucket.upload_file(fileSource, fileDestination)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922887c5",
   "metadata": {},
   "source": [
    "This functions takes in a list of WAV file names, and the name of the rec file the annotations need to be stored in, and then creates the corresponding spectrograms and rec file for the WAV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_rec_file(dataset, file_prefix):\n",
    "    lst_file_name = f\"{file_prefix}.lst\"\n",
    "    remove_Lst_fileIfOpen(lst_file_name)\n",
    "    create_lst_file(dataset, lst_file_name)\n",
    "    create_rec_file(lst_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8aaada",
   "metadata": {},
   "source": [
    "Remember to call cleanup before any call to final_rec_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all the spectrograms in the data folder\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f77f7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates spectrograms and rec file\n",
    "final_rec_file(train_set, \"train_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copies rec file from here to the bucket\n",
    "copy_to_bucket(\"train_full.rec\", \"train/train.rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rec_file(validation_set, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0978ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_bucket(\"val.rec\", \"validation/validation.rec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
